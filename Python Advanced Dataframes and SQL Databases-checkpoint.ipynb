{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas (cont.)\n",
    "Recall that while the general python syntax selectors identify column and then Row, with Pandas, the loc accessor identifies row and then column.\n",
    "\n",
    "Quick review of previous Panda skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Read in filename and set the index: election\n",
    "election = pd.read_csv(filename, index_col='county')\n",
    "\n",
    "# Create a separate dataframe with the columns ['winner', 'total', 'voters']: results\n",
    "results = election[['winner', 'total', 'voters']]\n",
    "\n",
    "# Print the output of results.head()\n",
    "print(results.head())\n",
    "\n",
    "# Slice the row labels 'Perry' to 'Potter': p_counties\n",
    "p_counties = election['Perry':'Potter']\n",
    "\n",
    "# Print the p_counties DataFrame\n",
    "print(p_counties)\n",
    "\n",
    "# Slice the row labels 'Potter' to 'Perry' in reverse order: p_counties_rev\n",
    "p_counties_rev = p_counties['Potter':'Perry':-1]\n",
    "\n",
    "# Print the p_counties_rev DataFrame\n",
    "print(p_counties_rev)\n",
    "\n",
    "\n",
    "# Slice the columns from the starting column to 'Obama': left_columns\n",
    "left_columns = election.loc[:,:'Obama']\n",
    "\n",
    "# Print the output of left_columns.head()\n",
    "print(left_columns.head())\n",
    "\n",
    "# Slice the columns from 'Obama' to 'winner': middle_columns\n",
    "middle_columns = election.loc[:,'Obama':'winner']\n",
    "\n",
    "# Print the output of middle_columns.head()\n",
    "print(middle_columns.head())\n",
    "\n",
    "# Slice the columns from 'Romney' to the end: 'right_columns'\n",
    "right_columns = election.loc[:,'Romney':]\n",
    "\n",
    "# Print the output of right_columns.head()\n",
    "print(right_columns.head())\n",
    "\n",
    "# Create a new dataframe with a subselection\n",
    "# Create the list of row labels: rows\n",
    "rows = ['Philadelphia', 'Centre', 'Fulton']\n",
    "\n",
    "# Create the list of column labels: cols\n",
    "cols = ['winner', 'Obama', 'Romney']\n",
    "\n",
    "# Create the new DataFrame: three_counties\n",
    "three_counties = election.loc[rows, cols]\n",
    "\n",
    "# Print the three_counties DataFrame\n",
    "print(three_counties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter dataframes\n",
    "# Create the boolean array: high_turnout\n",
    "high_turnout = election.turnout > 70\n",
    "\n",
    "# Filter the election DataFrame with the high_turnout array: high_turnout_df\n",
    "high_turnout_df = election[high_turnout]\n",
    "\n",
    "# Print the high_turnout_results DataFrame\n",
    "print(high_turnout_df)\n",
    "\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Create the boolean array: too_close\n",
    "too_close = election.margin < 1\n",
    "\n",
    "# Assign np.nan to the 'winner' column where the results were too close to call\n",
    "election.winner[too_close] = np.nan\n",
    "\n",
    "# Print the output of election.info()\n",
    "print(election.info())\n",
    "\n",
    "\n",
    "# Drop 0's and NaNs.\n",
    "# Select the 'age' and 'cabin' columns: df\n",
    "df = titanic[['age', 'cabin']]\n",
    "\n",
    "# Print the shape of df\n",
    "print(df.shape)\n",
    "\n",
    "# Drop rows where any of these columns contain missing data with how='any' and print the shape\n",
    "print(df.dropna(how='any').shape)\n",
    "\n",
    "# Drop rows in df where all of the columns have missing data with how='all' and print the shape\n",
    "print(df.dropna(how='all').shape)\n",
    "\n",
    "# Drop columns in titanic with less than 1000 non-missing values\n",
    "print(titanic.dropna(thresh=1000, axis='columns').info())\n",
    "\n",
    "\n",
    "# Use Apply method to extrapolate function across all elements using weather df.\n",
    "# Write a function to convert degrees Fahrenheit to degrees Celsius: to_celsius\n",
    "def to_celsius(F):\n",
    "    return 5/9*(F - 32)\n",
    "\n",
    "# Apply the function over 'Mean TemperatureF' and 'Mean Dew PointF': df_celsius\n",
    "df_celsius = weather[['Mean TemperatureF', 'Mean Dew PointF']].apply(to_celsius)\n",
    "\n",
    "# Reassign the columns df_celsius\n",
    "df_celsius.columns = ['Mean TemperatureC', 'Mean Dew PointC']\n",
    "\n",
    "# Print the output of df_celsius.head()\n",
    "print(df_celsius.head())\n",
    "\n",
    "\n",
    "# .map() function on a dictionary\n",
    "# Create the dictionary: red_vs_blue\n",
    "red_vs_blue = {'Obama':'blue', 'Romney':'red'}\n",
    "\n",
    "# Use the dictionary to map the 'winner' column to the new column: election['color']\n",
    "election['color'] = election.winner.map(red_vs_blue)\n",
    "\n",
    "# Print the output of election.head()\n",
    "print(election.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorized Functions\n",
    "When performance is an issue and you have alot of data, you want to avoid using for-loops such as apply and map.  Lets use the z-score function from sciPy to start. In statistics, the z-score is the number of standard deviations by which an observation is distancewise from the mean.  So if it is negative, it means the observation is below the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import zscore from scipy.stats\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Call zscore with election['turnout'] as input: turnout_zscore\n",
    "turnout_zscore = zscore(election.turnout)\n",
    "\n",
    "# Print the type of turnout_zscore\n",
    "print(type(turnout_zscore))\n",
    "\n",
    "# Assign turnout_zscore to a new column: election['turnout_zscore']\n",
    "election['turnout_zscore'] = turnout_zscore\n",
    "\n",
    "# Print the output of election.head()\n",
    "print(election.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Index Ninja\n",
    "Indexes are not mutable, so if you want to manipulate the index we need to understand how that is done. You can also index on multiple variables which allow you to dig in and select nested data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Changing Index of a dataframe\n",
    "# Create the list of new indexes: new_idx\n",
    "new_idx = [month.upper() for month in sales.index]\n",
    "\n",
    "# Assign new_idx to sales.index\n",
    "sales.index = new_idx\n",
    "\n",
    "# Print the sales DataFrame\n",
    "print(sales)\n",
    "\n",
    "# Assign the string 'MONTHS' to sales.index.name\n",
    "sales.index.name = 'MONTHS'\n",
    "\n",
    "# Print the sales DataFrame\n",
    "print(sales)\n",
    "\n",
    "# Assign the string 'PRODUCTS' to sales.columns.name \n",
    "sales.columns.name = 'PRODUCTS'\n",
    "\n",
    "# Print the sales dataframe again\n",
    "print(sales)\n",
    "\n",
    "# Build index, then a dataframe\n",
    "# Generate the list of months: months\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun']\n",
    "\n",
    "# Assign months to sales.index\n",
    "sales.index = months\n",
    "\n",
    "# Print the modified sales DataFrame\n",
    "print(sales)\n",
    "\n",
    "\n",
    "# Setting and sorting a MultiIndex\n",
    "# Set the index to be the columns ['state', 'month']: sales\n",
    "sales = sales.set_index(['state', 'month'])\n",
    "\n",
    "# Sort the MultiIndex: sales\n",
    "sales = sales.sort_index()\n",
    "\n",
    "# Print the sales DataFrame\n",
    "print(sales)\n",
    "\n",
    "\n",
    "# Digging into a MultiIndex dataset\n",
    "# Look up data for NY in month 1: NY_month1\n",
    "NY_month1 = sales.loc[('NY', 1)]\n",
    "\n",
    "# Look up data for CA and TX in month 2: CA_TX_month2\n",
    "CA_TX_month2 = sales.loc[(['CA', 'TX'], 2),:]\n",
    "\n",
    "# Look up data for all states in month 2: all_month2\n",
    "all_month2 = sales.loc[(slice(None), 2), :]\n",
    "\n",
    "\n",
    "# pivoting dataframes\n",
    "# Pivot users with signups indexed by weekday and city: signups_pivot\n",
    "signups_pivot = users.pivot(index='weekday', columns='city', values='signups')\n",
    "\n",
    "# Print signups_pivot\n",
    "print(signups_pivot)\n",
    "\n",
    "# Pivot users pivoted by both signups and visitors: pivot\n",
    "pivot = users.pivot(index='weekday', columns='city')\n",
    "\n",
    "# Print the pivoted DataFrame\n",
    "print(pivot)\n",
    "\n",
    "# We cannot pivot on multiIndexed data, so instead we first need to unstack the data so it only contains a single index.\n",
    "# Unstack users by 'weekday': byweekday\n",
    "byweekday = users.unstack(level='weekday')\n",
    "\n",
    "# Print the byweekday DataFrame\n",
    "print(byweekday)\n",
    "\n",
    "# Stack byweekday by 'weekday' and print it\n",
    "print(byweekday.stack(level='weekday'))\n",
    "\n",
    "\n",
    "# Use staking to make the two datasets match.\n",
    "# Stack 'city' back into the index of bycity: newusers\n",
    "newusers = bycity.stack(level='city')\n",
    "\n",
    "# Swap the levels of the index of newusers: newusers\n",
    "newusers = newusers.swaplevel(0, 1)\n",
    "\n",
    "# Print newusers and verify that the index is not sorted\n",
    "print(newusers)\n",
    "\n",
    "# Sort the index of newusers: newusers\n",
    "newusers = newusers.sort_index()\n",
    "\n",
    "# Print newusers and verify that the index is now sorted\n",
    "print(newusers)\n",
    "\n",
    "# Verify that the new DataFrame is equal to the original\n",
    "print(newusers.equals(users))\n",
    "\n",
    "\n",
    "# Melting\n",
    "# Reset the index: visitors_by_city_weekday\n",
    "visitors_by_city_weekday = visitors_by_city_weekday.reset_index() \n",
    "\n",
    "# Print visitors_by_city_weekday\n",
    "print(visitors_by_city_weekday)\n",
    "\n",
    "# Melt visitors_by_city_weekday: visitors\n",
    "visitors = pd.melt(visitors_by_city_weekday, id_vars=['weekday'], value_name='visitors')\n",
    "\n",
    "# Print visitors\n",
    "print(visitors)\n",
    "\n",
    "\n",
    "# Combine vistitors and signups into a single column\n",
    "# Melt users: skinny\n",
    "skinny = pd.melt(users, id_vars=['weekday', 'city'], value_vars=['visitors', 'signups'])\n",
    "\n",
    "# Print skinny\n",
    "print(skinny)\n",
    "\n",
    "# Create the DataFrame with the appropriate pivot table: signups_and_visitors\n",
    "signups_and_visitors = users.pivot_table(index='weekday', aggfunc=sum)\n",
    "\n",
    "# Print signups_and_visitors\n",
    "print(signups_and_visitors)\n",
    "\n",
    "# Add in the margins: signups_and_visitors_total \n",
    "signups_and_visitors_total = users.pivot_table(index='weekday', aggfunc=sum, margins=True)\n",
    "\n",
    "# Print signups_and_visitors_total\n",
    "print(signups_and_visitors_total)\n",
    "\n",
    "\n",
    "# GROUPBY\n",
    "# Group titanic by 'pclass'\n",
    "by_class = titanic.groupby('pclass')\n",
    "\n",
    "# Aggregate 'survived' column of by_class by count\n",
    "count_by_class = by_class.survived.count()\n",
    "\n",
    "# Print count_by_class\n",
    "print(count_by_class)\n",
    "\n",
    "# Group titanic by 'embarked' and 'pclass'\n",
    "by_mult = titanic.groupby(['embarked', 'pclass'])\n",
    "\n",
    "# Aggregate 'survived' column of by_mult by count\n",
    "count_mult = by_mult.survived.count()\n",
    "\n",
    "# Print count_mult\n",
    "print(count_mult)\n",
    "\n",
    "\n",
    "# Grouping by another series.  Make the index 'country' for both datasets.  Then we can bring the data together.\n",
    "# Read life_fname into a DataFrame: life\n",
    "life = pd.read_csv(life_fname, index_col='Country')\n",
    "\n",
    "# Read regions_fname into a DataFrame: regions\n",
    "regions = pd.read_csv(regions_fname, index_col='Country')\n",
    "\n",
    "# Group life by regions['region']: life_by_region\n",
    "life_by_region = life.groupby(regions['region'])\n",
    "\n",
    "# Print the mean over the '2010' column of life_by_region\n",
    "print(life_by_region['2010'].mean())\n",
    "\n",
    "\n",
    "# Compute aggregates of multiple columns\n",
    "# Group titanic by 'pclass': by_class\n",
    "by_class = titanic.groupby('pclass')\n",
    "\n",
    "# Select 'age' and 'fare'\n",
    "by_class_sub = by_class[['age','fare']]\n",
    "\n",
    "# Aggregate by_class_sub by 'max' and 'median': aggregated\n",
    "aggregated = by_class_sub.agg(['max', 'median'])\n",
    "\n",
    "# Print the maximum age in each class\n",
    "print(aggregated.loc[:, ('age','max')])\n",
    "\n",
    "# Print the median fare in each class\n",
    "print(aggregated.loc[:,('fare','median')])\n",
    "\n",
    "# Aggregating on index level\n",
    "# Read the CSV file into a DataFrame and sort the index: gapminder\n",
    "gapminder = pd.read_csv('gapminder.csv', index_col=['Year', 'region', 'Country']).sort_index()\n",
    "\n",
    "# Group gapminder by 'Year' and 'region': by_year_region\n",
    "by_year_region = gapminder.groupby(level=['Year', 'region'])\n",
    "\n",
    "# Define the function to compute spread: spread\n",
    "def spread(series):\n",
    "    return series.max() - series.min()\n",
    "\n",
    "# Create the dictionary: aggregator\n",
    "aggregator = {'population':'sum', 'child_mortality':'mean', 'gdp':spread}\n",
    "\n",
    "# Aggregate by_year_region using the dictionary: aggregated\n",
    "aggregated = by_year_region.agg(aggregator)\n",
    "\n",
    "# Print the last 6 entries of aggregated \n",
    "print(aggregated.tail(6))\n",
    "\n",
    "\n",
    "# Grouping on the function of an index.  For this example you need to understand strftime function. \n",
    "# This example will take the date and indentify the day of the week using the directive %a.\n",
    "# https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\n",
    "\n",
    "# Read file: sales\n",
    "sales = pd.read_csv('sales.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# Create a groupby object: by_day\n",
    "by_day = sales.groupby(sales.index.strftime('%a'))\n",
    "\n",
    "# Create sum: units_sum\n",
    "units_sum = by_day['Units'].sum()\n",
    "# Print units_sum\n",
    "print(units_sum)\n",
    "\n",
    "\n",
    "# Transformation\n",
    "# Detect Outliers with z-scores to identify countries from the dataset that have extreme life and fertility rates.\n",
    "# Import zscore\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Group gapminder_2010: standardized\n",
    "standardized = gapminder_2010.groupby('region')[['life', 'fertility']].transform(zscore)\n",
    "\n",
    "# Construct a Boolean Series to identify outliers: outliers\n",
    "outliers = (standardized['life'] < -3) | (standardized['fertility'] > 3)\n",
    "\n",
    "# Filter gapminder_2010 by the outliers: gm_outliers\n",
    "gm_outliers = gapminder_2010.loc[outliers]\n",
    "\n",
    "# Print gm_outliers\n",
    "print(gm_outliers)\n",
    "\n",
    "\n",
    "# Fill in missing data by group.  Find median and impute.\n",
    "# Create a groupby object: by_sex_class\n",
    "by_sex_class = titanic.groupby(['sex', 'pclass'])\n",
    "\n",
    "# Write a function that imputes median\n",
    "def impute_median(series):\n",
    "    return series.fillna(series.median())\n",
    "\n",
    "# Impute age and assign to titanic['age']\n",
    "titanic.age = by_sex_class['age'].transform(impute_median)\n",
    "\n",
    "# Print the output of titanic.tail(10)\n",
    "print(titanic.tail(10))\n",
    "\n",
    "\n",
    "# Transform using .apply\n",
    "def disparity(gr):\n",
    "    # Compute the spread of gr['gdp']: s\n",
    "    s = gr['gdp'].max() - gr['gdp'].min()\n",
    "    # Compute the z-score of gr['gdp'] as (gr['gdp']-gr['gdp'].mean())/gr['gdp'].std(): z\n",
    "    z = (gr['gdp'] - gr['gdp'].mean())/gr['gdp'].std()\n",
    "    # Return a DataFrame with the inputs {'z(gdp)':z, 'regional spread(gdp)':s}\n",
    "    return pd.DataFrame({'z(gdp)':z , 'regional spread(gdp)':s})\n",
    "\n",
    "# Group gapminder_2010 by 'region': regional\n",
    "regional = gapminder_2010.groupby('region')\n",
    "\n",
    "# Apply the disparity function on regional: reg_disp\n",
    "reg_disp = regional.apply(disparity)\n",
    "\n",
    "# Print the disparity of 'United States', 'United Kingdom', and 'China'\n",
    "print(reg_disp.loc[['United States', 'United Kingdom', 'China']])\n",
    "\n",
    "\n",
    "# Group and filter using .apply\n",
    "def c_deck_survival(gr):\n",
    "\n",
    "    c_passengers = gr['cabin'].str.startswith('C').fillna(False)\n",
    "\n",
    "    return gr.loc[c_passengers, 'survived'].mean()\n",
    "\n",
    "# Create a groupby object using titanic over the 'sex' column: by_sex\n",
    "by_sex = titanic.groupby('sex')\n",
    "\n",
    "# Call by_sex.apply with the function c_deck_survival\n",
    "c_surv_by_sex = by_sex.apply(c_deck_survival)\n",
    "\n",
    "# Print the survival rates\n",
    "print(c_surv_by_sex)\n",
    "\n",
    "\n",
    "# Grouping and filtering using .filter()\n",
    "# Read the CSV file into a DataFrame: sales\n",
    "sales = pd.read_csv('sales.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# Group sales by 'Company': by_company\n",
    "by_company = sales.groupby('Company')\n",
    "\n",
    "# Compute the sum of the 'Units' of by_company: by_com_sum\n",
    "by_com_sum = by_company['Units'].sum()\n",
    "print(by_com_sum)\n",
    "\n",
    "# Filter 'Units' where the sum is > 35: by_com_filt\n",
    "by_com_filt = by_company.filter(lambda g:g['Units'].sum() > 35)\n",
    "print(by_com_filt)\n",
    "\n",
    "# Group and filtering using .map()\n",
    "# Create the Boolean Series: under10\n",
    "under10 = (titanic['age'] < 10).map({True: 'under 10', False: 'over 10'})\n",
    "\n",
    "# Group by under10 and compute the survival rate\n",
    "survived_mean_1 = titanic.groupby(under10)['survived'].mean()\n",
    "print(survived_mean_1)\n",
    "\n",
    "# Group by under10 and pclass and compute the survival rate\n",
    "survived_mean_2 = titanic.groupby([under10, 'pclass'])['survived'].mean()\n",
    "print(survived_mean_2)\n",
    "\n",
    "# Use .value_counts()\n",
    "# Select the 'NOC' column of medals: country_names\n",
    "country_names = medals['NOC']\n",
    "\n",
    "# Count the number of medals won by each country: medal_counts\n",
    "medal_counts = country_names.value_counts()\n",
    "\n",
    "# Print top 15 countries ranked by medals\n",
    "print(medal_counts.head(15))\n",
    "\n",
    "\n",
    "# Use Pivot table to count medals by type\n",
    "# Construct the pivot table: counted\n",
    "counted = medals.pivot_table(index='NOC', columns='Medal', values='Athlete', aggfunc='count')\n",
    "\n",
    "# Create the new column: counted['totals']\n",
    "counted['totals'] = counted.sum(axis='columns')\n",
    "\n",
    "# Sort counted by the 'totals' column\n",
    "counted = counted.sort_values(by='totals', ascending=False)\n",
    "\n",
    "# Print the top 15 rows of counted\n",
    "print(counted.head(15))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
